#!/usr/bin/env bash
set -euo pipefail

usage() {
  cat <<'EOF'
Usage:
  bash scripts/validate_cpu_omp_cuda.sh [options]

End-to-end validation for three backends (CPU / OMP / CUDA):
  1) Performance benchmark (repeat N times per backend)
  2) Accuracy outputs (CPU + OMP(n=1/4/8) + CUDA; CPU as reference)
  3) Accuracy report (Markdown) via post_analysis/accuracy_comparison.py
  4) Combined report (Markdown) generated by this script

Options:
  --project <name>           Project name (default: ccw)
  --tag <name>               Experiment tag (default: YYYYMMDD_HHMMSS)
  --out-root <dir>           Root output directory (default: output/experiments)

  --repeat <N>               Benchmark repeat count (default: 3)
  --omp-threads <list>       OpenMP thread counts (-n) for OMP runs, comma-separated
                             (default: 1,4,8)
  --end <days>               Override END (days) in cfg.para for all runs (default: use input/<project>/<project>.cfg.para)

  --bench-io <off|all>       Benchmark output mode (default: off)
  --acc-io <off|all>         Accuracy output mode (default: all)

  --tol-omp <float>          Accuracy tol for CPU vs OMP rel_max (default: 1e-10)
  --tol-cuda <float>         Accuracy tol for CPU vs CUDA rel_max (default: 1e-6)
  --plot                     Generate accuracy plots (requires matplotlib)
  --plot-top <N>             Also plot top-N worst files (default: 3)

  --cuda-precond <default|on|off|auto>
                             Exported as SHUD_CUDA_PRECOND for CUDA runs (bench + accuracy)
  --cuda-precond-fp <default|fp64|fp32>
                             Exported as SHUD_CUDA_PRECOND_FP for CUDA runs (bench + accuracy)
  --profile <none|nsys|nvprof>
                             Profiling mode (CUDA only; benchmark step only)

  --strict-fp <0|1>           Export SHUD_STRICT_FP for all runs (optional)
  --det-reduce <0|1>          Export SHUD_DETERMINISTIC_REDUCE for all runs (optional)

  --no-omp                   Skip OMP backend
  --no-cuda                  Skip CUDA backend

  --skip-bench               Skip benchmark step
  --skip-accuracy            Skip accuracy step (runs + report)
  --skip-report              Skip combined report generation

Examples:
  # Default: benchmark with IO off, accuracy with full outputs
  bash scripts/validate_cpu_omp_cuda.sh --project ccw

  # OMP thread sweep + benchmark repeated 5x
  bash scripts/validate_cpu_omp_cuda.sh --project ccw --omp-threads 1,4,8,16 --repeat 5

  # Evaluate CUDA mixed-precision preconditioner (fp32) end-to-end
  bash scripts/validate_cpu_omp_cuda.sh --project ccw --cuda-precond on --cuda-precond-fp fp32
EOF
}

die() {
  echo "ERROR: $*" >&2
  exit 2
}

PROJECT="ccw"
TAG=""
OUT_ROOT="output/experiments"
REPEAT=3
OMP_THREADS_CSV="1,4,8"
END_DAYS=""
BENCH_IO="off"
ACC_IO="all"
TOL_OMP="1e-10"
TOL_CUDA="1e-6"
PLOT=0
PLOT_TOP=3
CUDA_PRECOND="default"
CUDA_PRECOND_FP="default"
PROFILE="none"
STRICT_FP=""
DET_REDUCE=""
NO_OMP=0
NO_CUDA=0
SKIP_BENCH=0
SKIP_ACCURACY=0
SKIP_REPORT=0

while [[ $# -gt 0 ]]; do
  case "$1" in
    --project) PROJECT="${2:-}"; shift 2 ;;
    --tag) TAG="${2:-}"; shift 2 ;;
    --out-root) OUT_ROOT="${2:-}"; shift 2 ;;
    --repeat) REPEAT="${2:-}"; shift 2 ;;
    --omp-threads) OMP_THREADS_CSV="${2:-}"; shift 2 ;;
    --end) END_DAYS="${2:-}"; shift 2 ;;
    --bench-io) BENCH_IO="${2:-}"; shift 2 ;;
    --acc-io) ACC_IO="${2:-}"; shift 2 ;;
    --tol-omp) TOL_OMP="${2:-}"; shift 2 ;;
    --tol-cuda) TOL_CUDA="${2:-}"; shift 2 ;;
    --plot) PLOT=1; shift ;;
    --plot-top) PLOT_TOP="${2:-}"; shift 2 ;;
    --cuda-precond) CUDA_PRECOND="${2:-}"; shift 2 ;;
    --cuda-precond-fp) CUDA_PRECOND_FP="${2:-}"; shift 2 ;;
    --profile) PROFILE="${2:-}"; shift 2 ;;
    --strict-fp) STRICT_FP="${2:-}"; shift 2 ;;
    --det-reduce) DET_REDUCE="${2:-}"; shift 2 ;;
    --no-omp) NO_OMP=1; shift ;;
    --no-cuda) NO_CUDA=1; shift ;;
    --skip-bench) SKIP_BENCH=1; shift ;;
    --skip-accuracy) SKIP_ACCURACY=1; shift ;;
    --skip-report) SKIP_REPORT=1; shift ;;
    -h|--help) usage; exit 0 ;;
    *) echo "ERROR: unknown arg: $1" >&2; usage; exit 2 ;;
  esac
done

[[ -n "${PROJECT}" ]] || die "--project is required"
[[ -n "${BENCH_IO}" ]] || die "--bench-io requires a non-empty value"
[[ -n "${ACC_IO}" ]] || die "--acc-io requires a non-empty value"

case "${BENCH_IO}" in
  off|none|all) ;;
  *) die "invalid --bench-io '${BENCH_IO}' (expect off|none|all)" ;;
esac
case "${ACC_IO}" in
  off|none|all) ;;
  *) die "invalid --acc-io '${ACC_IO}' (expect off|none|all)" ;;
esac

if [[ -z "${TAG}" ]]; then
  TAG="$(date +%Y%m%d_%H%M%S)"
fi
if ! [[ "${REPEAT}" =~ ^[0-9]+$ ]] || [[ "${REPEAT}" -lt 1 ]]; then
  die "--repeat must be a positive integer"
fi
if [[ -n "${END_DAYS}" ]]; then
  if ! [[ "${END_DAYS}" =~ ^[0-9]+([.][0-9]+)?$ ]]; then
    die "--end must be a number (days), got: '${END_DAYS}'"
  fi
fi
if ! [[ "${PLOT_TOP}" =~ ^[0-9]+$ ]] || [[ "${PLOT_TOP}" -lt 1 ]]; then
  die "--plot-top must be a positive integer"
fi
case "${CUDA_PRECOND}" in
  default|on|off|auto) ;;
  *) die "invalid --cuda-precond '${CUDA_PRECOND}' (expect default|on|off|auto)" ;;
esac
case "${CUDA_PRECOND_FP}" in
  default|fp64|fp32) ;;
  *) die "invalid --cuda-precond-fp '${CUDA_PRECOND_FP}' (expect default|fp64|fp32)" ;;
esac
case "${PROFILE}" in
  none|nsys|nvprof) ;;
  *) die "invalid --profile '${PROFILE}' (expect none|nsys|nvprof)" ;;
esac
if [[ -n "${STRICT_FP}" ]]; then
  case "${STRICT_FP}" in
    0|1) ;;
    *) die "--strict-fp must be 0 or 1" ;;
  esac
fi
if [[ -n "${DET_REDUCE}" ]]; then
  case "${DET_REDUCE}" in
    0|1) ;;
    *) die "--det-reduce must be 0 or 1" ;;
  esac
fi

parse_omp_threads_list() {
  local s="$1"
  s="${s//,/ }"
  local -a toks=()
  read -r -a toks <<<"${s}"

  local -a out=()
  local tok existing seen
  for tok in "${toks[@]}"; do
    [[ -n "${tok}" ]] || continue
    if ! [[ "${tok}" =~ ^[0-9]+$ ]] || [[ "${tok}" -lt 1 ]]; then
      die "--omp-threads contains invalid value: '${tok}'"
    fi
    seen=0
    for existing in "${out[@]}"; do
      if [[ "${existing}" == "${tok}" ]]; then
        seen=1
        break
      fi
    done
    if [[ "${seen}" -eq 0 ]]; then
      out+=( "${tok}" )
    fi
  done

  if [[ "${#out[@]}" -eq 0 ]]; then
    die "--omp-threads list is empty"
  fi
  printf '%s\n' "${out[@]}"
}

OMP_THREADS_LIST=()
OMP_THREADS_DISPLAY="<disabled>"
if [[ "${NO_OMP}" -eq 0 ]]; then
  mapfile -t OMP_THREADS_LIST < <(parse_omp_threads_list "${OMP_THREADS_CSV}")
  OMP_THREADS_DISPLAY="$(IFS=,; echo "${OMP_THREADS_LIST[*]}")"
fi

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "${SCRIPT_DIR}/.." && pwd)"
cd "${ROOT_DIR}"

EXP_DIR="${OUT_ROOT}/${PROJECT}/${TAG}"
mkdir -p "${EXP_DIR}"

META_MD="${EXP_DIR}/meta.md"
{
  echo "# SHUD CPU/OMP/CUDA validation bundle"
  echo ""
  echo "- Project: \`${PROJECT}\`"
  echo "- Tag: \`${TAG}\`"
  echo "- Root: \`${EXP_DIR}\`"
  echo "- Generated: \`$(date -u +%Y-%m-%dT%H:%M:%SZ)\`"
  echo ""

  echo "## Host"
  echo ""
  echo "- Hostname: \`$(hostname 2>/dev/null || echo unknown)\`"
  echo "- Kernel: \`$(uname -srvm 2>/dev/null || echo unknown)\`"
  if command -v lscpu >/dev/null 2>&1; then
    cpu_model="$(lscpu 2>/dev/null | awk -F: '/Model name/ {gsub(/^[[:space:]]+/, "", $2); print $2; exit}')"
    if [[ -n "${cpu_model:-}" ]]; then
      echo "- CPU: \`${cpu_model}\`"
    fi
  fi
  if command -v nvidia-smi >/dev/null 2>&1; then
    echo "- GPU:"
    nvidia-smi -L 2>/dev/null | sed 's/^/  - /' || true
  fi
  echo ""

  if command -v git >/dev/null 2>&1 && git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
    echo "## Git"
    echo ""
    echo "- Commit: \`$(git rev-parse HEAD)\`"
    echo "- Branch: \`$(git branch --show-current 2>/dev/null || true)\`"
    echo ""
  fi

  echo "## Config"
  echo ""
  echo "- repeat: \`${REPEAT}\`"
  echo "- omp_threads: \`${OMP_THREADS_DISPLAY}\`"
  echo "- bench_io: \`${BENCH_IO}\`"
  echo "- acc_io: \`${ACC_IO}\`"
  echo "- tol_omp: \`${TOL_OMP}\`"
  echo "- tol_cuda: \`${TOL_CUDA}\`"
  if [[ -n "${END_DAYS}" ]]; then
    echo "- end_days: \`${END_DAYS}\`"
  fi
  echo "- cuda_precond: \`${CUDA_PRECOND}\`"
  echo "- cuda_precond_fp: \`${CUDA_PRECOND_FP}\`"
  echo "- profile: \`${PROFILE}\`"
  if [[ -n "${STRICT_FP}" ]]; then
    echo "- strict_fp: \`${STRICT_FP}\`"
  fi
  if [[ -n "${DET_REDUCE}" ]]; then
    echo "- det_reduce: \`${DET_REDUCE}\`"
  fi
  echo ""
} >"${META_MD}"

COMMON_ENV=()
if [[ -n "${STRICT_FP}" ]]; then
  COMMON_ENV+=( SHUD_STRICT_FP="${STRICT_FP}" )
fi
if [[ -n "${DET_REDUCE}" ]]; then
  COMMON_ENV+=( SHUD_DETERMINISTIC_REDUCE="${DET_REDUCE}" )
fi

CUDA_ENV=()
if [[ "${CUDA_PRECOND}" != "default" ]]; then
  case "${CUDA_PRECOND}" in
    on) CUDA_ENV+=( SHUD_CUDA_PRECOND=1 ) ;;
    off) CUDA_ENV+=( SHUD_CUDA_PRECOND=0 ) ;;
    auto) CUDA_ENV+=( SHUD_CUDA_PRECOND=auto ) ;;
  esac
fi
if [[ "${CUDA_PRECOND_FP}" != "default" ]]; then
  case "${CUDA_PRECOND_FP}" in
    fp64) CUDA_ENV+=( SHUD_CUDA_PRECOND_FP=fp64 ) ;;
    fp32) CUDA_ENV+=( SHUD_CUDA_PRECOND_FP=fp32 ) ;;
  esac
fi

require_bin() {
  local path="$1"
  if [[ ! -x "${path}" ]]; then
    echo "ERROR: missing executable: ${path}" >&2
    return 1
  fi
}

extract_kv() {
  local key="$1"
  local line="$2"
  awk -v key="${key}" '{
    for (i = 1; i <= NF; i++) {
      split($i, kv, "=")
      if (kv[1] == key) { print kv[2]; exit }
    }
  }' <<<"${line}"
}

write_cfg_para() {
  local template="$1"
  local out_cfg="$2"
  local num_openmp="$3"
  local end_days="$4"
  local binary_output="$5"
  local ascii_output="$6"

  awk -v nt="${num_openmp}" -v end="${end_days}" -v binout="${binary_output}" -v ascout="${ascii_output}" '
    BEGIN { did_nt=0; did_end=0; did_bin=0; did_asc=0 }
    /^[[:space:]]*NUM_OPENMP[[:space:]]/ {
      printf "NUM_OPENMP\t%s\n", nt;
      did_nt=1;
      next
    }
    /^[[:space:]]*BINARY_OUTPUT[[:space:]]/ {
      if (binout != "") {
        printf "BINARY_OUTPUT\t%s\n", binout;
        did_bin=1;
        next
      }
    }
    /^[[:space:]]*ASCII_OUTPUT[[:space:]]/ {
      if (ascout != "") {
        printf "ASCII_OUTPUT\t%s\n", ascout;
        did_asc=1;
        next
      }
    }
    /^[[:space:]]*END[[:space:]]/ {
      if (end != "") {
        printf "END\t%s\n", end;
        did_end=1;
        next
      }
    }
    { print }
    END {
      if (did_nt == 0) {
        printf "NUM_OPENMP\t%s\n", nt;
      }
      if (binout != "" && did_bin == 0) {
        printf "BINARY_OUTPUT\t%s\n", binout;
      }
      if (ascout != "" && did_asc == 0) {
        printf "ASCII_OUTPUT\t%s\n", ascout;
      }
      if (end != "" && did_end == 0) {
        printf "END\t%s\n", end;
      }
    }
  ' "${template}" >"${out_cfg}"
}

write_project_file() {
  local out_prj="$1"
  local project="$2"
  local outpath="$3"
  local para_path="$4"

  cat >"${out_prj}" <<EOF
PRJ	${project}
OUTPATH	${outpath}
PARA	${para_path}
EOF
}

bench_run_one() {
  local backend="$1"
  local run_idx="$2"
  local bin="$3"

  local omp_threads=""
  if [[ "${backend}" =~ ^omp_n([0-9]+)$ ]]; then
    omp_threads="${BASH_REMATCH[1]}"
  fi
  local num_openmp="1"
  if [[ -n "${omp_threads}" ]]; then
    num_openmp="${omp_threads}"
  fi

  local run_dir="${BENCH_DIR}/${PROJECT}/${backend}"
  mkdir -p "${run_dir}"

  local run_tag
  run_tag="$(printf "run_%03d" "${run_idx}")"

  local log_file="${run_dir}/${run_tag}.log"
  local time_file="${run_dir}/${run_tag}.time"
  local out_dir="${run_dir}/${run_tag}.out"
  local prj_file="${run_dir}/${run_tag}.SHUD"
  local cfg_file="${run_dir}/${run_tag}.cfg.para"

  rm -rf "${out_dir}"
  mkdir -p "${out_dir}"

  template_cfg="input/${PROJECT}/${PROJECT}.cfg.para"
  if [[ ! -f "${template_cfg}" ]]; then
    echo "ERROR: missing template cfg.para: ${template_cfg}" >&2
    return 2
  fi

  bench_binout=""
  bench_ascout=""
  if [[ "${BENCH_IO}" == "off" || "${BENCH_IO}" == "none" ]]; then
    bench_binout="0"
    bench_ascout="0"
  fi

  write_cfg_para "${template_cfg}" "${cfg_file}" "${num_openmp}" "${END_DAYS}" "${bench_binout}" "${bench_ascout}"
  write_project_file "${prj_file}" "${PROJECT}" "${out_dir}" "${cfg_file}"

  local -a cmd=( "${bin}" -p "${prj_file}" "${PROJECT}" )

  local -a cmd_prefix=()
  if [[ "${backend}" == "cuda" && "${PROFILE}" != "none" ]]; then
    case "${PROFILE}" in
      nsys)
        if command -v nsys >/dev/null 2>&1; then
          cmd_prefix=( nsys profile --force-overwrite=true -o "${run_dir}/${run_tag}_nsys" )
        else
          echo "WARN: --profile nsys requested but 'nsys' not found; continuing without profiler" >&2
        fi
        ;;
      nvprof)
        if command -v nvprof >/dev/null 2>&1; then
          cmd_prefix=( nvprof -o "${run_dir}/${run_tag}.nvprof" )
        else
          echo "WARN: --profile nvprof requested but 'nvprof' not found; continuing without profiler" >&2
        fi
        ;;
    esac
  fi

  local -a env_prefix=("${COMMON_ENV[@]}")
  if [[ "${backend}" == "cuda" ]]; then
    env_prefix+=("${CUDA_ENV[@]}")
  fi

  echo "[run] bench backend=${backend} ${run_tag}: ${cmd_prefix[*]:-} ${cmd[*]}" >&2
  : >"${log_file}"
  set +e
  /usr/bin/time -f "%e" -o "${time_file}" env "${env_prefix[@]}" "${cmd_prefix[@]}" "${cmd[@]}" >>"${log_file}" 2>&1
  local rc=$?
  set -e
  if [[ "${rc}" -ne 0 ]]; then
    echo "WARN: bench backend=${backend} ${run_tag} exited with code ${rc} (see ${log_file})" >&2
  fi

  local wall_s=""
  if [[ -f "${time_file}" ]]; then
    wall_s="$(tr -d '[:space:]' <"${time_file}" || true)"
  fi

  local cvode_stats_line bench_stats_line
  cvode_stats_line="$(grep "CVODE_STATS" "${log_file}" | tail -n 1 | sed 's/^.*CVODE_STATS/CVODE_STATS/' || true)"
  bench_stats_line="$(grep "BENCH_STATS" "${log_file}" | tail -n 1 | sed 's/^.*BENCH_STATS/BENCH_STATS/' || true)"

  local nfe nli nni netf npe nps precond_fp
  nfe="$(extract_kv nfe "${cvode_stats_line}")"
  nli="$(extract_kv nli "${cvode_stats_line}")"
  nni="$(extract_kv nni "${cvode_stats_line}")"
  netf="$(extract_kv netf "${cvode_stats_line}")"
  npe="$(extract_kv npe "${cvode_stats_line}")"
  nps="$(extract_kv nps "${cvode_stats_line}")"
  precond_fp="$(extract_kv precond_fp "${cvode_stats_line}")"

  local bench_wall bench_cvode bench_io bench_forcing
  bench_wall="$(extract_kv wall_s "${bench_stats_line}")"
  bench_cvode="$(extract_kv cvode_s "${bench_stats_line}")"
  bench_io="$(extract_kv io_s "${bench_stats_line}")"
  bench_forcing="$(extract_kv forcing_s "${bench_stats_line}")"

  local rhs_calls rhs_kernels rhs_launch_us rhs_graph cuda_graph_mode strict_fp det_reduce
  rhs_calls="$(extract_kv rhs_calls "${bench_stats_line}")"
  rhs_kernels="$(extract_kv rhs_kernels "${bench_stats_line}")"
  rhs_launch_us="$(extract_kv rhs_launch_us "${bench_stats_line}")"
  rhs_graph="$(extract_kv rhs_graph "${bench_stats_line}")"
  cuda_graph_mode="$(extract_kv cuda_graph_mode "${bench_stats_line}")"
  strict_fp="$(extract_kv strict_fp "${bench_stats_line}")"
  det_reduce="$(extract_kv det_reduce "${bench_stats_line}")"

  printf "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" \
    "${backend}" \
    "${run_idx}" \
    "${wall_s}" \
    "${bench_wall}" \
    "${bench_cvode}" \
    "${bench_io}" \
    "${bench_forcing}" \
    "${rhs_calls}" \
    "${rhs_kernels}" \
    "${rhs_launch_us}" \
    "${rhs_graph}" \
    "${cuda_graph_mode}" \
    "${strict_fp}" \
    "${det_reduce}" \
    "${nfe}" \
    "${nli}" \
    "${nni}" \
    "${netf}" \
    "${npe}" \
    "${nps}" \
    "${precond_fp}" \
    "${BENCH_IO}" \
    "${CUDA_PRECOND}" \
    "${CUDA_PRECOND_FP}" \
    "${log_file}"
}

run_accuracy_backend() {
  local backend="$1" label="$2" bin="$3" out_dir="$4" log_file="$5" time_file="$6"
  shift 6
  local extra_args=( "$@" )
  local num_openmp="1"
  local omp_threads=""
  if [[ "${backend}" =~ ^omp_n([0-9]+)$ ]]; then
    omp_threads="${BASH_REMATCH[1]}"
    num_openmp="${omp_threads}"
  fi

  if [[ ! -x "${bin}" ]]; then
    if [[ "${backend}" == "cpu" ]]; then
      echo "ERROR: missing executable for CPU reference: ${bin}" >&2
      return 2
    fi
    echo "WARN: skip accuracy ${label} (missing executable: ${bin})" >&2
    rm -f "${log_file}" "${time_file}" || true
    rm -rf "${out_dir}" || true
    return 1
  fi

  rm -rf "${out_dir}"
  mkdir -p "${out_dir}"
  mkdir -p "$(dirname "${log_file}")"

  local prj_file="${ACC_DIR}/runfiles/${PROJECT}_${backend}.SHUD"
  local cfg_file="${ACC_DIR}/runfiles/${PROJECT}_${backend}.cfg.para"
  mkdir -p "${ACC_DIR}/runfiles"

  template_cfg="input/${PROJECT}/${PROJECT}.cfg.para"
  if [[ ! -f "${template_cfg}" ]]; then
    echo "ERROR: missing template cfg.para: ${template_cfg}" >&2
    return 2
  fi

  acc_binout=""
  acc_ascout=""
  if [[ "${ACC_IO}" == "off" || "${ACC_IO}" == "none" ]]; then
    acc_binout="0"
    acc_ascout="0"
  else
    acc_binout="1"
    acc_ascout="0"
  fi

  write_cfg_para "${template_cfg}" "${cfg_file}" "${num_openmp}" "${END_DAYS}" "${acc_binout}" "${acc_ascout}"
  write_project_file "${prj_file}" "${PROJECT}" "${out_dir}" "${cfg_file}"

  local -a cmd=( "${bin}" -p "${prj_file}" )
  cmd+=( "${extra_args[@]}" )
  cmd+=( "${PROJECT}" )

  local -a env_prefix=("${COMMON_ENV[@]}")
  if [[ "${backend}" == "cuda" ]]; then
    env_prefix+=("${CUDA_ENV[@]}")
  fi

  echo "[run] accuracy ${label}: ${cmd[*]}"
  : >"${log_file}"
  set +e
  /usr/bin/time -f "%e" -o "${time_file}" env "${env_prefix[@]}" "${cmd[@]}" >>"${log_file}" 2>&1
  local rc=$?
  set -e
  if [[ "${rc}" -ne 0 ]]; then
    echo "WARN: accuracy ${label} exited with code ${rc} (see ${log_file})" >&2
  fi
  return "${rc}"
}

EXIT_CODE=0

BENCH_DIR="${EXP_DIR}/bench"
ACC_DIR="${EXP_DIR}/accuracy"
BENCH_LOG="${BENCH_DIR}/${PROJECT}/bench.log"
BENCH_SUMMARY="${BENCH_DIR}/${PROJECT}/bench_summary.md"
ACC_REPORT="${ACC_DIR}/accuracy_report.md"
ACC_REPORT_CUDA="${ACC_DIR}/accuracy_report_cuda.md"
REPORT_MD="${EXP_DIR}/report.md"

if [[ "${SKIP_BENCH}" -eq 0 ]]; then
  echo "[step] benchmark -> ${BENCH_DIR}"
  mkdir -p "${BENCH_DIR}/${PROJECT}"
  printf "backend\trun\twall_s\trun_wall_s\tcvode_s\tio_s\tforcing_s\trhs_calls\trhs_kernels\trhs_launch_us\trhs_graph\tcuda_graph_mode\tstrict_fp\tdet_reduce\tnfe\tnli\tnni\tnetf\tnpe\tnps\tprecond_fp\tio_groups\tcuda_precond\tcuda_precond_fp\tlog\n" >"${BENCH_LOG}"

  BENCH_BACKENDS=( cpu )
  if [[ "${NO_OMP}" -eq 0 ]]; then
    for t in "${OMP_THREADS_LIST[@]}"; do
      BENCH_BACKENDS+=( "omp_n${t}" )
    done
  fi
  if [[ "${NO_CUDA}" -eq 0 ]]; then BENCH_BACKENDS+=( cuda ); fi

  for backend in "${BENCH_BACKENDS[@]}"; do
    bin=""
    case "${backend}" in
      cpu) bin="./shud" ;;
      omp_n*) bin="./shud_omp" ;;
      cuda) bin="./shud_cuda" ;;
      *) die "internal error: unknown benchmark backend key: ${backend}" ;;
    esac

    if ! require_bin "${bin}"; then
      EXIT_CODE=1
      if [[ "${backend}" == "cpu" ]]; then break; fi
      echo "WARN: skip benchmark backend=${backend} (missing ${bin})" >&2
      continue
    fi

    for ((i = 1; i <= REPEAT; i++)); do
      bench_run_one "${backend}" "${i}" "${bin}" >>"${BENCH_LOG}"
    done
  done

  if [[ -f "${BENCH_LOG}" ]]; then
    python3 - <<'PY' "${BENCH_LOG}" "${BENCH_SUMMARY}" "${PROJECT}"
import math
import re
import sys
from collections import defaultdict

bench_log, out_md, project = sys.argv[1:4]

rows = []
with open(bench_log, "r", encoding="utf-8") as f:
  header = f.readline().rstrip("\n").split("\t")
  for line in f:
    line = line.rstrip("\n")
    if not line:
      continue
    parts = line.split("\t")
    rows.append(dict(zip(header, parts)))

def to_float(x):
  try:
    return float(x)
  except Exception:
    return None

by_backend = defaultdict(list)
for r in rows:
  by_backend[r["backend"]].append(r)

def mean_std(vals):
  vals = [v for v in vals if v is not None]
  if not vals:
    return None, None
  m = sum(vals) / len(vals)
  if len(vals) < 2:
    return m, 0.0
  var = sum((v - m) ** 2 for v in vals) / (len(vals) - 1)
  return m, math.sqrt(var)

def sort_key(backend: str):
  if backend == "cpu":
    return (0, 0, backend)
  m = re.match(r"^omp_n(\d+)$", backend)
  if m:
    return (1, int(m.group(1)), backend)
  if backend == "cuda":
    return (2, 0, backend)
  return (9, 0, backend)

with open(out_md, "w", encoding="utf-8") as f:
  f.write(f"# SHUD benchmark summary ({project})\n\n")
  f.write(f"Source: `{bench_log}`\n\n")
  for backend in sorted(by_backend.keys(), key=sort_key):
    rs = by_backend[backend]
    wall = [to_float(r["wall_s"]) for r in rs]
    cvode = [to_float(r["cvode_s"]) for r in rs]
    io = [to_float(r["io_s"]) for r in rs]
    forcing = [to_float(r["forcing_s"]) for r in rs]
    rhs_kernels = [to_float(r.get("rhs_kernels")) for r in rs]
    rhs_launch_us = [to_float(r.get("rhs_launch_us")) for r in rs]

    wall_m, wall_s = mean_std(wall)
    cvode_m, cvode_s = mean_std(cvode)
    io_m, io_s = mean_std(io)
    forcing_m, forcing_s = mean_std(forcing)
    rhs_kernels_m, rhs_kernels_s = mean_std(rhs_kernels)
    rhs_launch_us_m, rhs_launch_us_s = mean_std(rhs_launch_us)

    f.write(f"## {backend}\n\n")
    f.write("| metric | mean (s) | stdev (s) |\n")
    f.write("|---|---:|---:|\n")
    if wall_m is not None:
      f.write(f"| wall | {wall_m:.3f} | {wall_s:.3f} |\n")
    if cvode_m is not None:
      f.write(f"| cvode | {cvode_m:.3f} | {cvode_s:.3f} |\n")
    if io_m is not None:
      f.write(f"| io | {io_m:.3f} | {io_s:.3f} |\n")
    if forcing_m is not None:
      f.write(f"| forcing | {forcing_m:.3f} | {forcing_s:.3f} |\n")
    if rhs_kernels_m is not None:
      f.write(f"| rhs kernels/call | {rhs_kernels_m:.3f} | {rhs_kernels_s:.3f} |\n")
    if rhs_launch_us_m is not None:
      f.write(f"| rhs launch (us/call) | {rhs_launch_us_m:.3f} | {rhs_launch_us_s:.3f} |\n")

    # CVODE stats: show last run's values for quick reference
    last = rs[-1]
    stats = {k: last.get(k) for k in ("nfe", "nli", "nni", "netf", "npe", "nps")}
    if any(v for v in stats.values()):
      f.write("\nCVODE stats (last run):\n\n")
      f.write("```\n")
      f.write(" ".join([f"{k}={v}" for k, v in stats.items() if v]) + "\n")
      f.write("```\n\n")
PY
  fi
else
  echo "[skip] benchmark"
fi

if [[ "${SKIP_ACCURACY}" -eq 0 ]]; then
  echo "[step] accuracy outputs -> ${ACC_DIR}/outputs"
  mkdir -p "${ACC_DIR}/outputs" "${ACC_DIR}/logs"

  cpu_dir="${ACC_DIR}/outputs/${PROJECT}_cpu"
  cuda_dir="${ACC_DIR}/outputs/${PROJECT}_cuda"

  cpu_log="${ACC_DIR}/logs/${PROJECT}_cpu.log"
  cpu_time="${ACC_DIR}/logs/${PROJECT}_cpu.time"
  cuda_log="${ACC_DIR}/logs/${PROJECT}_cuda.log"
  cuda_time="${ACC_DIR}/logs/${PROJECT}_cuda.time"

  set +e
  run_accuracy_backend "cpu" "CPU serial" "./shud" "${cpu_dir}" "${cpu_log}" "${cpu_time}"
  rc=$?
  set -e
  if [[ "${rc}" -ne 0 ]]; then
    EXIT_CODE=1
  fi

  if [[ "${NO_OMP}" -eq 0 ]]; then
    for t in "${OMP_THREADS_LIST[@]}"; do
      omp_key="omp_n${t}"
      omp_dir="${ACC_DIR}/outputs/${PROJECT}_${omp_key}"
      omp_log="${ACC_DIR}/logs/${PROJECT}_${omp_key}.log"
      omp_time="${ACC_DIR}/logs/${PROJECT}_${omp_key}.time"

      set +e
      run_accuracy_backend "${omp_key}" "OpenMP (n=${t})" "./shud_omp" "${omp_dir}" "${omp_log}" "${omp_time}"
      rc=$?
      set -e
      if [[ "${rc}" -ne 0 ]]; then
        EXIT_CODE=1
      fi
    done
  else
    rm -rf "${ACC_DIR}/outputs/${PROJECT}_omp_"* || true
  fi

  if [[ "${NO_CUDA}" -eq 0 ]]; then
    set +e
    run_accuracy_backend "cuda" "CUDA" "./shud_cuda" "${cuda_dir}" "${cuda_log}" "${cuda_time}"
    rc=$?
    set -e
    if [[ "${rc}" -ne 0 ]]; then
      EXIT_CODE=1
    fi
  else
    rm -rf "${cuda_dir}" || true
  fi

  echo "[step] accuracy reports -> ${ACC_DIR}"
  skip_omp_dir="${ACC_DIR}/outputs/__skip_omp__"
  skip_cuda_dir="${ACC_DIR}/outputs/__skip_cuda__"
  rm -rf "${skip_omp_dir}" "${skip_cuda_dir}" || true

  cuda_arg_dir="${cuda_dir}"
  if [[ "${NO_CUDA}" -ne 0 ]]; then
    cuda_arg_dir="${skip_cuda_dir}"
  fi

  if [[ "${NO_OMP}" -eq 0 ]]; then
    for t in "${OMP_THREADS_LIST[@]}"; do
      omp_key="omp_n${t}"
      omp_dir="${ACC_DIR}/outputs/${PROJECT}_${omp_key}"
      out_md="${ACC_DIR}/accuracy_report_${omp_key}.md"

      if [[ ! -d "${omp_dir}" ]]; then
        echo "WARN: missing OMP outputs for n=${t}; skipping report ${out_md}" >&2
        EXIT_CODE=1
        continue
      fi

      plot_args=()
      if [[ "${PLOT}" -eq 1 ]]; then
        plot_args=( --plot --plot-dir "${ACC_DIR}/plots/${omp_key}" --plot-top "${PLOT_TOP}" )
      fi

      echo "[step] accuracy report -> ${out_md}"
      set +e
      python3 post_analysis/accuracy_comparison.py \
        --project "${PROJECT}" \
        --cpu "${cpu_dir}" \
        --omp "${omp_dir}" \
        --cuda "${cuda_arg_dir}" \
        --out "${out_md}" \
        --tol-omp "${TOL_OMP}" \
        --tol-cuda "${TOL_CUDA}" \
        "${plot_args[@]}"
      rc=$?
      set -e
      if [[ "${rc}" -ne 0 ]]; then
        EXIT_CODE=1
      fi
    done
  fi

  if [[ "${NO_CUDA}" -eq 0 ]]; then
    if [[ ! -d "${cuda_dir}" ]]; then
      echo "WARN: missing CUDA outputs; skipping report ${ACC_REPORT_CUDA}" >&2
      EXIT_CODE=1
    else
      plot_args=()
      if [[ "${PLOT}" -eq 1 ]]; then
        plot_args=( --plot --plot-dir "${ACC_DIR}/plots/cuda" --plot-top "${PLOT_TOP}" )
      fi
      echo "[step] accuracy report -> ${ACC_REPORT_CUDA}"
      set +e
      python3 post_analysis/accuracy_comparison.py \
        --project "${PROJECT}" \
        --cpu "${cpu_dir}" \
        --omp "${skip_omp_dir}" \
        --cuda "${cuda_dir}" \
        --out "${ACC_REPORT_CUDA}" \
        --tol-omp "${TOL_OMP}" \
        --tol-cuda "${TOL_CUDA}" \
        "${plot_args[@]}"
      rc=$?
      set -e
      if [[ "${rc}" -ne 0 ]]; then
        EXIT_CODE=1
      fi
    fi
  fi

  echo "[step] accuracy summary -> ${ACC_REPORT}"
  set +e
  any_fail="$(
    python3 - <<'PY' "${ACC_DIR}" "${PROJECT}" "${ACC_REPORT}" "${TOL_OMP}" "${TOL_CUDA}"
from __future__ import annotations

import datetime as _dt
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass(frozen=True)
class Counts:
    passed: int
    failed: int
    missing: int
    error: int
    worst_file: Optional[str]
    worst_relmax: Optional[str]


def parse_counts(text: str, which: str) -> Optional[Counts]:
    m = re.search(
        rf"^- CPU vs {which}: PASS `(\d+)`, FAIL `(\d+)`, MISSING `(\d+)`, ERROR `(\d+)`\s*$",
        text,
        flags=re.M,
    )
    if not m:
        return None
    passed, failed, missing, error = (int(m.group(i)) for i in range(1, 5))
    worst_file = worst_relmax = None
    m2 = re.search(rf"^- Worst CPU vs {which}: `([^`]+)` \(`rel_max=([^)]+)`\)\s*$", text, flags=re.M)
    if m2:
        worst_file = m2.group(1).strip()
        worst_relmax = m2.group(2).strip()
    return Counts(
        passed=passed,
        failed=failed,
        missing=missing,
        error=error,
        worst_file=worst_file,
        worst_relmax=worst_relmax,
    )


def sort_backend(name: str) -> tuple[int, int, str]:
    if name == "cpu":
        return (0, 0, name)
    m = re.match(r"^omp_n(\d+)$", name)
    if m:
        return (1, int(m.group(1)), name)
    if name == "cuda":
        return (2, 0, name)
    return (9, 0, name)


def main() -> int:
    acc_dir_s, project, out_md_s, tol_omp_s, tol_cuda_s = sys.argv[1:6]
    acc_dir = Path(acc_dir_s)
    out_md = Path(out_md_s)
    now = _dt.datetime.now(tz=_dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

    cpu_dir = acc_dir / "outputs" / f"{project}_cpu"
    cuda_dir = acc_dir / "outputs" / f"{project}_cuda"

    omp_reports = sorted(
        acc_dir.glob("accuracy_report_omp_n*.md"),
        key=lambda p: sort_backend(p.stem.replace("accuracy_report_", "")),
    )
    cuda_report = acc_dir / "accuracy_report_cuda.md"
    if not cuda_report.exists():
        cuda_report = None

    omp_rows: list[tuple[str, int, Optional[Counts], Path]] = []
    for p in omp_reports:
        key = p.stem.replace("accuracy_report_", "")
        m = re.match(r"^omp_n(\d+)$", key)
        threads = int(m.group(1)) if m else 0
        text = p.read_text(encoding="utf-8", errors="replace")
        omp_rows.append((key, threads, parse_counts(text, "OMP"), p))

    cuda_counts: Optional[Counts] = None
    cuda_report_path: Optional[Path] = None
    if cuda_report is not None and cuda_report.exists():
        cuda_report_path = cuda_report
        text = cuda_report.read_text(encoding="utf-8", errors="replace")
        cuda_counts = parse_counts(text, "CUDA")
    elif omp_rows:
        # Fallback: reuse CUDA summary from any OMP report that includes it.
        _, _, _, p0 = omp_rows[0]
        text = p0.read_text(encoding="utf-8", errors="replace")
        cuda_counts = parse_counts(text, "CUDA")
        cuda_report_path = p0

    any_fail = False
    for _, _, c, _ in omp_rows:
        if c and (c.failed > 0 or c.missing > 0 or c.error > 0):
            any_fail = True
    if cuda_counts and (cuda_counts.failed > 0 or cuda_counts.missing > 0 or cuda_counts.error > 0):
        any_fail = True

    lines: list[str] = []
    lines.append("# SHUD Backend Accuracy Summary (CPU as reference)")
    lines.append("")
    lines.append(f"- Project: `{project}`")
    lines.append(f"- CPU dir: `{cpu_dir.as_posix()}`")
    lines.append(f"- CUDA dir: `{cuda_dir.as_posix() if cuda_dir.exists() else '<none>'}`")
    lines.append(f"- Generated: `{now}`")
    lines.append("")
    lines.append("## Tolerances (relative to max(|CPU|) per file)")
    lines.append("")
    lines.append(f"- CPU vs OMP: `rel_max <= {tol_omp_s}`")
    lines.append(f"- CPU vs CUDA: `rel_max <= {tol_cuda_s}`")
    lines.append("")
    lines.append("## Summary")
    lines.append("")
    lines.append("| Backend | Threads | PASS | FAIL | MISSING | ERROR | Worst | Report |")
    lines.append("|---|---:|---:|---:|---:|---:|---|---|")

    if not omp_rows:
        lines.append("| OMP | N/A | N/A | N/A | N/A | N/A | N/A | _none_ |")
    else:
        for key, threads, c, p in sorted(omp_rows, key=lambda x: sort_backend(x[0])):
            if c is None:
                lines.append(f"| OMP | {threads} | N/A | N/A | N/A | N/A | N/A | `{p.name}` |")
                any_fail = True
                continue
            worst = "N/A"
            if c.worst_file and c.worst_relmax:
                worst = f"`{c.worst_file}` (rel_max={c.worst_relmax})"
            lines.append(
                f"| OMP | {threads} | {c.passed} | {c.failed} | {c.missing} | {c.error} | {worst} | `{p.name}` |"
            )

    if cuda_counts is None:
        lines.append("| CUDA | N/A | N/A | N/A | N/A | N/A | N/A | _none_ |")
    else:
        worst = "N/A"
        if cuda_counts.worst_file and cuda_counts.worst_relmax:
            worst = f"`{cuda_counts.worst_file}` (rel_max={cuda_counts.worst_relmax})"
        report_cell = f"`{cuda_report_path.name}`" if cuda_report_path is not None else "N/A"
        lines.append(
            f"| CUDA | N/A | {cuda_counts.passed} | {cuda_counts.failed} | {cuda_counts.missing} | {cuda_counts.error} | {worst} | {report_cell} |"
        )

    out_md.parent.mkdir(parents=True, exist_ok=True)
    out_md.write_text("\n".join(lines) + "\n", encoding="utf-8")

    print("1" if any_fail else "0")
    return 0


raise SystemExit(main())
PY
  )"
  rc=$?
  set -e
  if [[ "${rc}" -ne 0 ]]; then
    EXIT_CODE=1
  elif [[ "${any_fail}" != "0" ]]; then
    EXIT_CODE=1
  fi
else
  echo "[skip] accuracy"
fi

if [[ "${SKIP_REPORT}" -eq 0 ]]; then
  echo "[step] combined report -> ${REPORT_MD}"
  python3 - <<'PY' "${EXP_DIR}" "${PROJECT}" "${BENCH_LOG}" "${BENCH_SUMMARY}" "${ACC_DIR}" "${TOL_OMP}" "${TOL_CUDA}" "${REPORT_MD}"
from __future__ import annotations

import csv
import datetime as _dt
import math
import re
import statistics
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple


def _try_float(v: Optional[str]) -> Optional[float]:
    if v is None:
        return None
    s = v.strip()
    if not s:
        return None
    try:
        x = float(s)
    except Exception:
        return None
    if not math.isfinite(x):
        return None
    return x


def _mean_std(values: list[Optional[float]]) -> tuple[Optional[float], Optional[float]]:
    vals = [v for v in values if v is not None]
    if not vals:
        return None, None
    if len(vals) == 1:
        return vals[0], 0.0
    return statistics.mean(vals), statistics.stdev(vals)


def _fmt_mean_std(m: Optional[float], s: Optional[float], *, digits: int = 3) -> str:
    if m is None:
        return "N/A"
    if s is None or s == 0.0:
        return f"{m:.{digits}f}"
    return f"{m:.{digits}f} Â± {s:.{digits}f}"


def _fmt_float(v: Optional[float], *, digits: int = 3) -> str:
    if v is None:
        return "N/A"
    return f"{v:.{digits}f}"


def _speedup(cpu_m: Optional[float], other_m: Optional[float]) -> Optional[float]:
    if cpu_m is None or other_m is None or other_m <= 0:
        return None
    return cpu_m / other_m


def _sort_backend(name: str) -> Tuple[int, int, str]:
    if name == "cpu":
        return (0, 0, name)
    m = re.match(r"^omp_n(\d+)$", name)
    if m:
        return (1, int(m.group(1)), name)
    if name == "cuda":
        return (2, 0, name)
    return (9, 0, name)


@dataclass(frozen=True)
class BenchBackendSummary:
    backend: str
    runs: int
    wall_m: Optional[float]
    wall_s: Optional[float]
    run_wall_m: Optional[float]
    run_wall_s: Optional[float]
    cvode_m: Optional[float]
    cvode_s: Optional[float]
    io_m: Optional[float]
    io_s: Optional[float]
    forcing_m: Optional[float]
    forcing_s: Optional[float]


def _read_bench_log(path: Path) -> list[dict[str, str]]:
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        return [dict(r) for r in reader]


def _summarize_bench(rows: list[dict[str, str]]) -> list[BenchBackendSummary]:
    by_backend: dict[str, list[dict[str, str]]] = {}
    for r in rows:
        backend = (r.get("backend") or "").strip()
        if not backend:
            continue
        by_backend.setdefault(backend, []).append(r)

    summaries: list[BenchBackendSummary] = []
    for backend in sorted(by_backend.keys(), key=_sort_backend):
        rs = by_backend[backend]
        wall = [_try_float(r.get("wall_s")) for r in rs]
        run_wall = [_try_float(r.get("run_wall_s")) for r in rs]
        cvode = [_try_float(r.get("cvode_s")) for r in rs]
        io = [_try_float(r.get("io_s")) for r in rs]
        forcing = [_try_float(r.get("forcing_s")) for r in rs]

        wall_m, wall_s = _mean_std(wall)
        run_wall_m, run_wall_s = _mean_std(run_wall)
        cvode_m, cvode_s = _mean_std(cvode)
        io_m, io_s = _mean_std(io)
        forcing_m, forcing_s = _mean_std(forcing)

        summaries.append(
            BenchBackendSummary(
                backend=backend,
                runs=len(rs),
                wall_m=wall_m,
                wall_s=wall_s,
                run_wall_m=run_wall_m,
                run_wall_s=run_wall_s,
                cvode_m=cvode_m,
                cvode_s=cvode_s,
                io_m=io_m,
                io_s=io_s,
                forcing_m=forcing_m,
                forcing_s=forcing_s,
            )
        )

    return summaries


@dataclass(frozen=True)
class CompareCounts:
    passed: int
    failed: int
    missing: int
    error: int
    worst_file: Optional[str]
    worst_relmax: Optional[str]


def _parse_counts(text: str, which: str) -> Optional[CompareCounts]:
    m = re.search(
        rf"^- CPU vs {which}: PASS `(\d+)`, FAIL `(\d+)`, MISSING `(\d+)`, ERROR `(\d+)`\s*$",
        text,
        flags=re.M,
    )
    if not m:
        return None
    passed, failed, missing, error = (int(m.group(i)) for i in range(1, 5))
    worst_file = worst_relmax = None
    m2 = re.search(rf"^- Worst CPU vs {which}: `([^`]+)` \(`rel_max=([^)]+)`\)\s*$", text, flags=re.M)
    if m2:
        worst_file = m2.group(1).strip()
        worst_relmax = m2.group(2).strip()
    return CompareCounts(
        passed=passed,
        failed=failed,
        missing=missing,
        error=error,
        worst_file=worst_file,
        worst_relmax=worst_relmax,
    )


def main() -> int:
    exp_dir_s, project, bench_log_s, bench_summary_s, acc_dir_s, tol_omp_s, tol_cuda_s, out_s = sys.argv[1:9]
    exp_dir = Path(exp_dir_s)
    bench_log = Path(bench_log_s)
    bench_summary = Path(bench_summary_s)
    acc_dir = Path(acc_dir_s)
    out_path = Path(out_s)

    now = _dt.datetime.now(tz=_dt.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

    bench_summaries: list[BenchBackendSummary] = []
    if bench_log.exists():
        try:
            bench_summaries = _summarize_bench(_read_bench_log(bench_log))
        except Exception:
            bench_summaries = []

    cpu_wall = next((s.wall_m for s in bench_summaries if s.backend == "cpu"), None)
    cpu_run_wall = next((s.run_wall_m for s in bench_summaries if s.backend == "cpu"), None)

    acc_summary = acc_dir / "accuracy_report.md"
    omp_reports = sorted(
        acc_dir.glob("accuracy_report_omp_n*.md"),
        key=lambda p: _sort_backend(p.stem.replace("accuracy_report_", "")),
    )
    cuda_report = acc_dir / "accuracy_report_cuda.md"
    cuda_report = cuda_report if cuda_report.exists() else None

    lines: list[str] = []
    lines.append("# SHUD backend validation report")
    lines.append("")
    lines.append(f"- Project: `{project}`")
    lines.append(f"- Bundle: `{exp_dir.as_posix()}`")
    lines.append(f"- Generated: `{now}`")
    lines.append("")

    lines.append("## Artifacts")
    lines.append("")
    lines.append(f"- meta.md: `{(exp_dir / 'meta.md').as_posix()}`")
    lines.append(f"- bench.log: `{bench_log.as_posix()}`" + ("" if bench_log.exists() else " (missing)"))
    lines.append(
        f"- bench_summary.md: `{bench_summary.as_posix()}`" + ("" if bench_summary.exists() else " (missing)")
    )
    lines.append(
        f"- accuracy_report.md: `{acc_summary.as_posix()}`" + ("" if acc_summary.exists() else " (missing)")
    )
    lines.append("")

    lines.append("## Performance (benchmark)")
    lines.append("")
    if not bench_summaries:
        lines.append("_No benchmark data found._")
        lines.append("")
    else:
        lines.append("| Backend | Runs | wall_s (/usr/bin/time) | Speedup | run_wall_s (BENCH_STATS) | Speedup | cvode_s | io_s | forcing_s |")
        lines.append("|---|---:|---:|---:|---:|---:|---:|---:|---:|")
        for s in bench_summaries:
            sp_wall = _speedup(cpu_wall, s.wall_m)
            sp_run_wall = _speedup(cpu_run_wall, s.run_wall_m)
            lines.append(
                "| {backend} | {runs} | {wall} | {sp_wall} | {run_wall} | {sp_run_wall} | {cvode} | {io} | {forcing} |".format(
                    backend=s.backend,
                    runs=s.runs,
                    wall=_fmt_mean_std(s.wall_m, s.wall_s),
                    sp_wall=_fmt_float(sp_wall, digits=2),
                    run_wall=_fmt_mean_std(s.run_wall_m, s.run_wall_s),
                    sp_run_wall=_fmt_float(sp_run_wall, digits=2),
                    cvode=_fmt_mean_std(s.cvode_m, s.cvode_s),
                    io=_fmt_mean_std(s.io_m, s.io_s),
                    forcing=_fmt_mean_std(s.forcing_m, s.forcing_s),
                )
            )
        lines.append("")

    lines.append("## Accuracy (CPU as reference)")
    lines.append("")
    if not omp_reports and cuda_report is None and not acc_summary.exists():
        lines.append("_No accuracy reports found._")
        lines.append("")
    else:
        lines.append("- Tolerances:")
        lines.append(f"  - CPU vs OMP: `rel_max <= {tol_omp_s}`")
        lines.append(f"  - CPU vs CUDA: `rel_max <= {tol_cuda_s}`")
        lines.append("")

        rows: list[tuple[str, int, Optional[CompareCounts], str]] = []
        for p in omp_reports:
            key = p.stem.replace("accuracy_report_", "")
            m = re.match(r"^omp_n(\d+)$", key)
            threads = int(m.group(1)) if m else 0
            text = p.read_text(encoding="utf-8", errors="replace")
            rows.append((key, threads, _parse_counts(text, "OMP"), p.as_posix()))

        cuda_counts: Optional[CompareCounts] = None
        cuda_report_path: Optional[str] = None
        if cuda_report is not None:
            cuda_report_path = cuda_report.as_posix()
            text = cuda_report.read_text(encoding="utf-8", errors="replace")
            cuda_counts = _parse_counts(text, "CUDA")
        elif rows:
            cuda_report_path = rows[0][3]
            text = Path(cuda_report_path).read_text(encoding="utf-8", errors="replace")
            cuda_counts = _parse_counts(text, "CUDA")

        lines.append("| Backend | Threads | PASS | FAIL | MISSING | ERROR | Worst | Report |")
        lines.append("|---|---:|---:|---:|---:|---:|---|---|")
        if rows:
            for key, threads, c, path in sorted(rows, key=lambda x: _sort_backend(x[0])):
                if c is None:
                    lines.append(f"| OMP | {threads} | N/A | N/A | N/A | N/A | N/A | `{path}` |")
                    continue
                worst = "N/A"
                if c.worst_file and c.worst_relmax:
                    worst = f"`{c.worst_file}` (rel_max={c.worst_relmax})"
                lines.append(
                    f"| OMP | {threads} | {c.passed} | {c.failed} | {c.missing} | {c.error} | {worst} | `{path}` |"
                )
        else:
            lines.append("| OMP | N/A | N/A | N/A | N/A | N/A | N/A | _none_ |")

        if cuda_counts is None:
            lines.append("| CUDA | N/A | N/A | N/A | N/A | N/A | N/A | _none_ |")
        else:
            worst = "N/A"
            if cuda_counts.worst_file and cuda_counts.worst_relmax:
                worst = f"`{cuda_counts.worst_file}` (rel_max={cuda_counts.worst_relmax})"
            lines.append(
                f"| CUDA | N/A | {cuda_counts.passed} | {cuda_counts.failed} | {cuda_counts.missing} | {cuda_counts.error} | {worst} | `{cuda_report_path}` |"
            )
        lines.append("")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
    return 0


raise SystemExit(main())
PY
else
  echo "[skip] report"
fi

echo "[done] ${EXP_DIR}"
echo "  - meta:     ${META_MD}"
if [[ "${SKIP_BENCH}" -eq 0 ]]; then
  echo "  - bench:    ${BENCH_SUMMARY}"
fi
if [[ "${SKIP_ACCURACY}" -eq 0 ]]; then
  echo "  - accuracy: ${ACC_REPORT}"
fi
if [[ "${SKIP_REPORT}" -eq 0 ]]; then
  echo "  - report:   ${REPORT_MD}"
fi

exit "${EXIT_CODE}"
